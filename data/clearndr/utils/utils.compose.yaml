volumes:
    arkime-logs:
        name: arkime-logs-{{ .Release.seed }}
    arkime-pcap:
        name: arkime-pcap-{{ .Release.seed }}
    arkime-config:
        name: arkime-config-{{ .Release.seed }}

services:
    {{- if .Values.evebox }}
    evebox:
        image: jasonish/evebox:{{ (.Values.evebox).version | default "master" }}
        container_name: {{template "base-name" .}}-evebox-{{ .Release.seed }}
        command: ['-e', 'http://elasticsearch:9200']
        restart: {{ .Values.globals.restartmode | default "unless-stopped" }}
        environment:
            - EVEBOX_HTTP_TLS_ENABLED=false
            - EVEBOX_AUTHENTICATION_REQUIRED=false
        networks:
            network:
    {{- end }}

    {{- if .Values.arkime }}
    arkime:
        image: ghcr.io/stamusnetworks/stamus-images/arkime:5.5
        container_name: {{template "base-name" .}}-arkime-{{ .Release.seed }}
        restart: {{ .Values.globals.restartmode | default "unless-stopped" }}
        depends_on:
            elasticsearch:
                condition: service_healthy
        healthcheck:
            test: ['CMD', 'curl', '-f', 'http://scirius:8000/arkime']
            interval: 15s
            timeout: 5s
            retries: 3
        volumes:
            - {{ .Release.location }}/containers-data/suricata/logs:/suricata-logs
            - {{ .Release.location }}/fpc:/suricata-logs/fpc
            - arkime-logs:/opt/arkime/logs
            - arkime-pcap:/opt/arkime/raw
        {{- if or .Values.global.exposePorts .Values.arkime.openPort }}
        depends_on:
            - suricata
        ports:
            - 8005:8005
        {{- end }}
        networks:
            network:
    {{- end }}

    ownership-fix:
        image: busybox
        container_name: {{template "base-name" .}}-ownership-fix-{{ .Release.seed }}
        entrypoint: /bin/sh -c
        command: [
            'chown -R {{.Release.user}}:{{.Release.group}} /containers-data/scirius &&
            chown -R {{.Release.user}}:{{.Release.group}} /containers-data/arkime &&
            chown -R {{.Release.user}}:{{.Release.group}} /containers-data/suricata/rules &&
            chmod -R 777 /containers-data/suricata'
        ]
        restart: {{ .Values.globals.restartmode | default "on-failure" }}
        volumes:
            - {{ .Release.location | default ".." }}/containers-data:/containers-data
        depends_on:
            - scirius
            - suricata

    cron:
        # This containers handles crontabs for the other containers, following the 1 task per container principle.
        # It is based on  `docker:latest` image, wich is an alpine image with docker binary
        container_name: {{template "base-name" .}}-cron-{{ .Release.seed }}
        image: docker:latest
        healthcheck:
            test: ['CMD', 'ps', '|', 'grep', 'crond']
            interval: 1m
            timeout: 1m
            retries: 3
        command:
            [
                sh,
                -c,
                "echo '*	*	 *	*	 *	run-parts /etc/periodic/1min' >> /etc/crontabs/root && crond -f -l 8",
            ]
        restart: ${RESTART_MODE:-unless-stopped}
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock # This bind-mout allows using the hosts docker deamon instead of created one inside the container

            # Those volumes will contain the cron jobs
            - ${PWD}/containers-data/cron-jobs/1min:/etc/periodic/1min/:ro
            - ${PWD}/containers-data/cron-jobs/15min:/etc/periodic/15min/:ro
            - ${PWD}/containers-data/cron-jobs/daily:/etc/periodic/daily/:ro
            - ${PWD}/containers-data/cron-jobs/hourly:/etc/periodic/hourly/:ro
            - ${PWD}/containers-data/cron-jobs/monthly:/etc/periodic/monthly/:ro
            - ${PWD}/containers-data/cron-jobs/weekly:/etc/periodic/weekly/:ro
